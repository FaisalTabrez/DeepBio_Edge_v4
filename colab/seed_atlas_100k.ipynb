{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68404fb8",
   "metadata": {},
   "source": [
    "# DeepBio-Scan: Large-Scale Atlas Seeding (N=100,000)\n",
    "## Phase 1: Reference Atlas Generation (Optimized)\n",
    "\n",
    "**Personas Active:**\n",
    "- `@Embedder-ML` (Model Logic & Inference)\n",
    "- `@Data-Ops` (Data Pipeline & Parquet Export)\n",
    "\n",
    "**Hardware Target:** Google Colab T4 GPU (or better)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a3c1e0",
   "metadata": {},
   "source": [
    "# DeepBio-Scan: Large-Scale Atlas Seeding (N=100,000)\n",
    "## Phase 1: Reference Atlas Generation\n",
    "\n",
    "**Personas Active:**\n",
    "- `@Embedder-ML` (Model Logic & Inference)\n",
    "- `@Data-Ops` (Data Pipeline & Parquet Export)\n",
    "\n",
    "**Hardware Target:** Google Colab T4 GPU (or better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53876c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Data-Ops: Dependency Setup\n",
    "!pip uninstall -y torch_xla\n",
    "!pip install --upgrade transformers==4.40.2 pandas pyarrow duckdb lancedb accelerate biopython tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193f7a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import Entrez, SeqIO\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# @Embedder-ML: GPU Acceleration Check\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on: {device.upper()}\")\n",
    "if device == \"cpu\":\n",
    "    print(\"WARNING: GPU not detected. Embedding will be slow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64136676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Data-Ops: Step 1 - High-Speed Batch-fetching logic using Bio.Entrez (FASTA Mode)\n",
    "Entrez.email = \"data-ops@deepbio.scan\"  # Replace with your email\n",
    "CHECKPOINT_FILE = \"checkpoint_atlas.parquet\"\n",
    "\n",
    "def fetch_taxonomy_metadata(tax_ids):\n",
    "    \"\"\"\n",
    "    @Data-Ops: Parallel Metadata Retrieval\n",
    "    Fetches taxonomy details for a list of TaxIDs.\n",
    "    \"\"\"\n",
    "    if not tax_ids:\n",
    "        return {}\n",
    "    \n",
    "    print(f\"Fetching taxonomy for {len(tax_ids)} unique TaxIDs...\")\n",
    "    tax_dict = {}\n",
    "    try:\n",
    "        # Fetch taxonomy records\n",
    "        handle = Entrez.efetch(db=\"taxonomy\", id=\",\".join(tax_ids), retmode=\"xml\")\n",
    "        records = Entrez.read(handle)\n",
    "        handle.close()\n",
    "        \n",
    "        for record in records:\n",
    "            tax_id = record.get(\"TaxId\")\n",
    "            lineage_ex = record.get(\"LineageEx\", [])\n",
    "            \n",
    "            phylum, class_name, order, family, genus = \"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\"\n",
    "            \n",
    "            for taxon in lineage_ex:\n",
    "                rank = taxon.get(\"Rank\")\n",
    "                name = taxon.get(\"ScientificName\")\n",
    "                if rank == \"phylum\": phylum = name\n",
    "                elif rank == \"class\": class_name = name\n",
    "                elif rank == \"order\": order = name\n",
    "                elif rank == \"family\": family = name\n",
    "                elif rank == \"genus\": genus = name\n",
    "                \n",
    "            tax_dict[tax_id] = {\n",
    "                \"Phylum\": phylum,\n",
    "                \"Class\": class_name,\n",
    "                \"Order\": order,\n",
    "                \"Family\": family,\n",
    "                \"Genus\": genus\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching taxonomy: {e}\")\n",
    "    \n",
    "    return tax_dict\n",
    "\n",
    "def fetch_marine_eukaryotes(target_count=100000, batch_size=500):\n",
    "    print(f\"Fetching {target_count} marine eukaryotic sequences...\")\n",
    "    \n",
    "    # Search query for marine eukaryotes (e.g., 18S/COI)\n",
    "    search_query = \"eukaryota[Organism] AND (marine[All Fields] OR ocean[All Fields]) AND (18S[All Fields] OR COI[All Fields])\"\n",
    "    \n",
    "    handle = Entrez.esearch(db=\"nucleotide\", term=search_query, retmax=target_count, usehistory=\"y\")\n",
    "    record = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    \n",
    "    webenv = record[\"WebEnv\"]\n",
    "    query_key = record[\"QueryKey\"]\n",
    "    total_found = int(record[\"Count\"])\n",
    "    print(f\"Found {total_found} sequences matching query. Fetching up to {target_count}...\")\n",
    "    \n",
    "    # @Data-Ops: Robustness - Resume Logic & Memory Optimization\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "    \n",
    "    # Find existing checkpoints to determine start_index\n",
    "    existing_files = [f for f in os.listdir(\"checkpoints\") if f.startswith(\"batch_\") and f.endswith(\".parquet\")]\n",
    "    if existing_files:\n",
    "        # Extract start indices from filenames\n",
    "        indices = [int(f.split('_')[1].split('.')[0]) for f in existing_files]\n",
    "        start_index = max(indices) + batch_size\n",
    "        print(f\"Found {len(existing_files)} existing batches. Resuming from index {start_index}...\")\n",
    "    else:\n",
    "        start_index = 0\n",
    "    \n",
    "    for start in range(start_index, min(target_count, total_found), batch_size):\n",
    "        print(f\"Fetching batch {start} to {start + batch_size}...\")\n",
    "        try:\n",
    "            # @Data-Ops: High-Speed Fetching (FASTA Mode)\n",
    "            fetch_handle = Entrez.efetch(\n",
    "                db=\"nucleotide\", \n",
    "                rettype=\"fasta\", \n",
    "                retmode=\"text\", \n",
    "                retstart=start, \n",
    "                retmax=batch_size, \n",
    "                webenv=webenv, \n",
    "                query_key=query_key\n",
    "            )\n",
    "            \n",
    "            batch_records = list(SeqIO.parse(fetch_handle, \"fasta\"))\n",
    "            fetch_handle.close()\n",
    "            \n",
    "            # Extract Accessions to fetch TaxIDs (FASTA doesn't have TaxIDs directly)\n",
    "            # @Data-Ops: Memory Optimization - Use esummary instead of full XML efetch\n",
    "            # Fetch Document Summaries to get TaxIDs (Lightweight)\n",
    "            docsum_handle = Entrez.esummary(\n",
    "                db=\"nucleotide\", \n",
    "                retstart=start, \n",
    "                retmax=batch_size, \n",
    "                webenv=webenv, \n",
    "                query_key=query_key\n",
    "            )\n",
    "            docsums = Entrez.read(docsum_handle)\n",
    "            docsum_handle.close()\n",
    "            \n",
    "            accession_to_taxid = {}\n",
    "            for docsum in docsums:\n",
    "                acc_version = docsum.get(\"AccessionVersion\", \"\")\n",
    "                acc = docsum.get(\"Caption\", \"\")\n",
    "                tax_id = str(docsum.get(\"TaxId\", \"Unknown\"))\n",
    "                \n",
    "                if acc_version:\n",
    "                    accession_to_taxid[acc_version.split('.')[0]] = tax_id\n",
    "                if acc:\n",
    "                    accession_to_taxid[acc.split('.')[0]] = tax_id\n",
    "            \n",
    "            # Get unique TaxIDs and fetch taxonomy\n",
    "            unique_taxids = list(set([tid for tid in accession_to_taxid.values() if tid != \"Unknown\"]))\n",
    "            taxonomy_metadata = fetch_taxonomy_metadata(unique_taxids)\n",
    "            \n",
    "            batch_data = []\n",
    "            for seq_record in batch_records:\n",
    "                accession = seq_record.id.split('.')[0]\n",
    "                sequence = str(seq_record.seq).upper()\n",
    "                \n",
    "                # @Data-Ops: Biological Filter (200bp - 2000bp to prevent OOM)\n",
    "                if len(sequence) < 200 or len(sequence) > 2000:\n",
    "                    continue\n",
    "                \n",
    "                # Try to extract scientific name from description (usually after the accession)\n",
    "                desc_parts = seq_record.description.split(' ', 1)\n",
    "                scientific_name = desc_parts[1] if len(desc_parts) > 1 else \"Unknown\"\n",
    "                \n",
    "                tax_id = accession_to_taxid.get(accession, \"Unknown\")\n",
    "                tax_info = taxonomy_metadata.get(tax_id, {})\n",
    "                \n",
    "                batch_data.append({\n",
    "                    \"AccessionID\": accession,\n",
    "                    \"ScientificName\": scientific_name,\n",
    "                    \"TaxID\": tax_id,\n",
    "                    \"Phylum\": tax_info.get(\"Phylum\", \"Unknown\"),\n",
    "                    \"Class\": tax_info.get(\"Class\", \"Unknown\"),\n",
    "                    \"Order\": tax_info.get(\"Order\", \"Unknown\"),\n",
    "                    \"Family\": tax_info.get(\"Family\", \"Unknown\"),\n",
    "                    \"Genus\": tax_info.get(\"Genus\", \"Unknown\"),\n",
    "                    \"Sequence\": sequence,\n",
    "                    \"Quality_Check\": True # Already filtered for >200bp\n",
    "                })\n",
    "            \n",
    "            # @Data-Ops: OOM Prevention - Save ONLY this batch\n",
    "            if batch_data:\n",
    "                df_batch = pd.DataFrame(batch_data)\n",
    "                batch_file = f\"checkpoints/batch_{start}.parquet\"\n",
    "                df_batch.to_parquet(batch_file, engine=\"pyarrow\")\n",
    "                print(f\"Batch saved: {batch_file} ({len(df_batch)} records).\")\n",
    "            else:\n",
    "                print(f\"No valid records in batch {start}.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching batch {start}: {e}\")\n",
    "            time.sleep(5) # Backoff\n",
    "            \n",
    "    # Return the list of batch files instead of loading them all into memory\n",
    "    print(\"Fetching complete. Batches are saved in the 'checkpoints' directory.\")\n",
    "    all_files = [os.path.join(\"checkpoints\", f) for f in os.listdir(\"checkpoints\") if f.startswith(\"batch_\") and f.endswith(\".parquet\")]\n",
    "    print(f\"Total batches: {len(all_files)}\")\n",
    "    return all_files\n",
    "\n",
    "# Execute fetching\n",
    "batch_files = fetch_marine_eukaryotes(target_count=100000, batch_size=500)\n",
    "print(f\"Dataset ready. {len(batch_files)} batches saved to disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674d8b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Embedder-ML: Step 2 - Neural Embedding Pipeline\n",
    "class LargeScaleEmbedder:\n",
    "    def __init__(self, model_name=\"InstaDeepAI/nucleotide-transformer-v2-50m-multi-species\"):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Initializing Model: {model_name} on {self.device}...\")\n",
    "        \n",
    "        # Load Config and Monkey-Patch\n",
    "        config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "        \n",
    "        # @Embedder-ML: Monkey-patch config.intermediate_size = 4096\n",
    "        config.intermediate_size = 4096\n",
    "        print(f\"Monkey-patched intermediate_size to: {config.intermediate_size}\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        \n",
    "        # @Embedder-ML: Reverted to AutoModelForMaskedLM because the custom model architecture requires it\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(\n",
    "            model_name, \n",
    "            config=config,\n",
    "            trust_remote_code=True,\n",
    "            ignore_mismatched_sizes=True\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        print(\"Model successfully loaded.\")\n",
    "\n",
    "    def embedding_generator(self, sequences, batch_size=8):\n",
    "        \"\"\"\n",
    "        Generator-based pipeline with batch_size=8 to prevent CUDA OOM.\n",
    "        Yields float32 vectors of exactly 768 dimensions.\n",
    "        \"\"\"\n",
    "        for i in tqdm(range(0, len(sequences), batch_size), desc=\"Embedding Batches\"):\n",
    "            batch = sequences[i:i+batch_size]\n",
    "            \n",
    "            # Clean sequences and FORCE truncation to 1000bp to guarantee memory safety\n",
    "            batch = [seq.upper().replace(\"\\n\", \"\").replace(\"\\r\", \"\").replace(\"N\", \"A\")[:1000] for seq in batch]\n",
    "            \n",
    "            inputs = self.tokenizer(\n",
    "                batch, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                max_length=1000\n",
    "            ).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Use output_hidden_states=True to get the embeddings from the MaskedLM model\n",
    "                outputs = self.model(**inputs, output_hidden_states=True)\n",
    "                \n",
    "                # Mean Pooling\n",
    "                last_hidden_state = outputs.hidden_states[-1]\n",
    "                attention_mask = inputs[\"attention_mask\"].unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "                \n",
    "                sum_embeddings = torch.sum(last_hidden_state * attention_mask, 1)\n",
    "                sum_mask = torch.clamp(attention_mask.sum(1), min=1e-9)\n",
    "                mean_embeddings = sum_embeddings / sum_mask\n",
    "                \n",
    "                # The 50m model has hidden_size=512. We need exactly 768 dimensions.\n",
    "                # We will pad with zeros to reach 768 dimensions.\n",
    "                current_dim = mean_embeddings.shape[1]\n",
    "                target_dim = 768\n",
    "                \n",
    "                if current_dim < target_dim:\n",
    "                    padding = torch.zeros((mean_embeddings.shape[0], target_dim - current_dim), device=self.device)\n",
    "                    mean_embeddings = torch.cat([mean_embeddings, padding], dim=1)\n",
    "                elif current_dim > target_dim:\n",
    "                    mean_embeddings = mean_embeddings[:, :target_dim]\n",
    "                \n",
    "                # Extract to CPU immediately\n",
    "                result = mean_embeddings.cpu().numpy().astype(np.float32)\n",
    "            \n",
    "            # @Embedder-ML: Clear CUDA cache BEFORE yielding to prevent fragmentation OOM\n",
    "            del inputs\n",
    "            del outputs\n",
    "            del last_hidden_state\n",
    "            del attention_mask\n",
    "            del sum_embeddings\n",
    "            del sum_mask\n",
    "            del mean_embeddings\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            yield result\n",
    "\n",
    "# Initialize Embedder\n",
    "embedder = LargeScaleEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420cf02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Data-Ops: Step 3 - Process Batches and Export\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import gc\n",
    "\n",
    "print(\"Starting large-scale embedding generation batch-by-batch...\")\n",
    "\n",
    "# Get all checkpoint files and sort them\n",
    "batch_files = [os.path.join(\"checkpoints\", f) for f in os.listdir(\"checkpoints\") if f.startswith(\"batch_\") and f.endswith(\".parquet\")]\n",
    "batch_files.sort(key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
    "\n",
    "output_file = \"reference_atlas_100k.parquet\"\n",
    "writer = None\n",
    "total_records = 0\n",
    "\n",
    "for batch_file in tqdm(batch_files, desc=\"Processing Checkpoints\"):\n",
    "    df_batch = pd.read_parquet(batch_file)\n",
    "    \n",
    "    if df_batch.empty:\n",
    "        continue\n",
    "        \n",
    "    # Generate embeddings for this batch\n",
    "    all_vectors = []\n",
    "    for batch_vectors in embedder.embedding_generator(df_batch[\"Sequence\"].tolist(), batch_size=8):\n",
    "        all_vectors.append(batch_vectors)\n",
    "        \n",
    "    final_vectors = np.concatenate(all_vectors, axis=0)\n",
    "    \n",
    "    # Add vectors to dataframe\n",
    "    df_batch[\"Vector\"] = list(final_vectors.astype(np.float32))\n",
    "    \n",
    "    # Convert to PyArrow Table\n",
    "    table = pa.Table.from_pandas(df_batch)\n",
    "    \n",
    "    # Initialize writer with the schema of the first batch\n",
    "    if writer is None:\n",
    "        writer = pq.ParquetWriter(output_file, table.schema)\n",
    "        \n",
    "    writer.write_table(table)\n",
    "    total_records += len(df_batch)\n",
    "    \n",
    "    # Free memory aggressively\n",
    "    del df_batch\n",
    "    del all_vectors\n",
    "    del final_vectors\n",
    "    del table\n",
    "    gc.collect()\n",
    "\n",
    "if writer:\n",
    "    writer.close()\n",
    "\n",
    "print(f\"SUCCESS: Atlas saved to {output_file}.\")\n",
    "print(f\"Total records processed and saved: {total_records}\")\n",
    "print(\"Ready for LanceDB ingestion.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
