{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31a3c1e0",
   "metadata": {},
   "source": [
    "# DeepBio-Scan: Large-Scale Atlas Seeding (N=100,000)\n",
    "## Phase 1: Reference Atlas Generation\n",
    "\n",
    "**Personas Active:**\n",
    "- `@Embedder-ML` (Model Logic & Inference)\n",
    "- `@Data-Ops` (Data Pipeline & Parquet Export)\n",
    "\n",
    "**Hardware Target:** Google Colab T4 GPU (or better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53876c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Data-Ops: Dependency Setup\n",
    "!pip uninstall -y torch_xla\n",
    "!pip install --upgrade transformers==4.40.2 pandas pyarrow duckdb lancedb accelerate biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193f7a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import Entrez\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig\n",
    "\n",
    "# @Embedder-ML: GPU Acceleration Check\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on: {device.upper()}\")\n",
    "if device == \"cpu\":\n",
    "    print(\"WARNING: GPU not detected. Embedding will be slow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64136676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Data-Ops: Step 1 - Batch-fetching logic using Bio.Entrez\n",
    "Entrez.email = \"data-ops@deepbio.scan\"  # Replace with your email\n",
    "\n",
    "def fetch_marine_eukaryotes(target_count=100000, batch_size=10000):\n",
    "    print(f\"Fetching {target_count} marine eukaryotic sequences...\")\n",
    "    \n",
    "    # Search query for marine eukaryotes (e.g., 18S/COI)\n",
    "    search_query = \"eukaryota[Organism] AND (marine[All Fields] OR ocean[All Fields]) AND (18S[All Fields] OR COI[All Fields])\"\n",
    "    \n",
    "    handle = Entrez.esearch(db=\"nucleotide\", term=search_query, retmax=target_count, usehistory=\"y\")\n",
    "    record = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    \n",
    "    webenv = record[\"WebEnv\"]\n",
    "    query_key = record[\"QueryKey\"]\n",
    "    total_found = int(record[\"Count\"])\n",
    "    print(f\"Found {total_found} sequences matching query. Fetching up to {target_count}...\")\n",
    "    \n",
    "    records_data = []\n",
    "    \n",
    "    for start in range(0, min(target_count, total_found), batch_size):\n",
    "        print(f\"Fetching batch {start} to {start + batch_size}...\")\n",
    "        try:\n",
    "            fetch_handle = Entrez.efetch(\n",
    "                db=\"nucleotide\", \n",
    "                retmode=\"xml\", \n",
    "                retstart=start, \n",
    "                retmax=batch_size, \n",
    "                webenv=webenv, \n",
    "                query_key=query_key\n",
    "            )\n",
    "            batch_records = Entrez.read(fetch_handle)\n",
    "            fetch_handle.close()\n",
    "            \n",
    "            for seq_record in batch_records:\n",
    "                # Extract metadata\n",
    "                accession = seq_record.get(\"GBSeq_primary-accession\", \"Unknown\")\n",
    "                scientific_name = seq_record.get(\"GBSeq_organism\", \"Unknown\")\n",
    "                sequence = seq_record.get(\"GBSeq_sequence\", \"\").upper()\n",
    "                \n",
    "                # Extract taxonomy\n",
    "                taxonomy = seq_record.get(\"GBSeq_taxonomy\", \"\")\n",
    "                tax_list = [t.strip() for t in taxonomy.split(\";\")] if taxonomy else []\n",
    "                \n",
    "                # Basic mapping (NCBI taxonomy can vary, this is a simplified mapping)\n",
    "                phylum = tax_list[1] if len(tax_list) > 1 else \"Unknown\"\n",
    "                class_name = tax_list[2] if len(tax_list) > 2 else \"Unknown\"\n",
    "                order = tax_list[3] if len(tax_list) > 3 else \"Unknown\"\n",
    "                family = tax_list[4] if len(tax_list) > 4 else \"Unknown\"\n",
    "                genus = tax_list[5] if len(tax_list) > 5 else scientific_name.split()[0] if scientific_name != \"Unknown\" else \"Unknown\"\n",
    "                \n",
    "                # Extract TaxID from feature qualifiers if available\n",
    "                tax_id = \"Unknown\"\n",
    "                for feature in seq_record.get(\"GBSeq_feature-table\", []):\n",
    "                    if feature.get(\"GBFeature_key\") == \"source\":\n",
    "                        for qual in feature.get(\"GBFeature_quals\", []):\n",
    "                            if qual.get(\"GBQualifier_name\") == \"db_xref\" and qual.get(\"GBQualifier_value\", \"\").startswith(\"taxon:\"):\n",
    "                                tax_id = qual.get(\"GBQualifier_value\").split(\":\")[1]\n",
    "                                break\n",
    "                \n",
    "                records_data.append({\n",
    "                    \"AccessionID\": accession,\n",
    "                    \"ScientificName\": scientific_name,\n",
    "                    \"TaxID\": tax_id,\n",
    "                    \"Phylum\": phylum,\n",
    "                    \"Class\": class_name,\n",
    "                    \"Order\": order,\n",
    "                    \"Family\": family,\n",
    "                    \"Genus\": genus,\n",
    "                    \"Sequence\": sequence,\n",
    "                    \"Quality_Check\": len(sequence) > 300\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching batch {start}: {e}\")\n",
    "            time.sleep(5) # Backoff\n",
    "            \n",
    "    df = pd.DataFrame(records_data)\n",
    "    print(f\"Successfully fetched {len(df)} sequences.\")\n",
    "    return df\n",
    "\n",
    "# Execute fetching (Uncomment to run actual fetch, using a small subset for testing if needed)\n",
    "# df_sequences = fetch_marine_eukaryotes(target_count=100000, batch_size=5000)\n",
    "\n",
    "# For demonstration, if not fetching 100k right now, we create a dummy dataframe with the correct schema\n",
    "print(\"Creating synthetic dataset for demonstration of the pipeline...\")\n",
    "df_sequences = pd.DataFrame({\n",
    "    \"AccessionID\": [f\"SEQ{i:06d}\" for i in range(1000)],\n",
    "    \"ScientificName\": [\"Grimpoteuthis sp.\"] * 500 + [\"Bathynomus giganteus\"] * 500,\n",
    "    \"TaxID\": [\"12345\"] * 1000,\n",
    "    \"Phylum\": [\"Mollusca\"] * 500 + [\"Arthropoda\"] * 500,\n",
    "    \"Class\": [\"Cephalopoda\"] * 500 + [\"Malacostraca\"] * 500,\n",
    "    \"Order\": [\"Octopoda\"] * 500 + [\"Isopoda\"] * 500,\n",
    "    \"Family\": [\"Opisthoteuthidae\"] * 500 + [\"Cirolanidae\"] * 500,\n",
    "    \"Genus\": [\"Grimpoteuthis\"] * 500 + [\"Bathynomus\"] * 500,\n",
    "    \"Sequence\": [\"ACGT\" * 100] * 500 + [\"TGCA\" * 50] * 500, # 400bp and 200bp\n",
    "})\n",
    "df_sequences[\"Quality_Check\"] = df_sequences[\"Sequence\"].apply(lambda x: len(x) > 300)\n",
    "print(f\"Dataset ready. Shape: {df_sequences.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674d8b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Embedder-ML: Step 2 - Neural Embedding Pipeline\n",
    "class LargeScaleEmbedder:\n",
    "    def __init__(self, model_name=\"InstaDeepAI/nucleotide-transformer-v2-50m-multi-species\"):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Initializing Model: {model_name} on {self.device}...\")\n",
    "        \n",
    "        # Load Config and Monkey-Patch\n",
    "        config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "        \n",
    "        # @Embedder-ML: Monkey-patch config.intermediate_size = 4096\n",
    "        config.intermediate_size = 4096\n",
    "        print(f\"Monkey-patched intermediate_size to: {config.intermediate_size}\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(\n",
    "            model_name, \n",
    "            config=config,\n",
    "            trust_remote_code=True\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        print(\"Model successfully loaded.\")\n",
    "\n",
    "    def embedding_generator(self, sequences, batch_size=64):\n",
    "        \"\"\"\n",
    "        Generator-based pipeline with batch_size=64.\n",
    "        Yields float32 vectors of exactly 768 dimensions.\n",
    "        \"\"\"\n",
    "        for i in range(0, len(sequences), batch_size):\n",
    "            batch = sequences[i:i+batch_size]\n",
    "            \n",
    "            # Clean sequences\n",
    "            batch = [seq.upper().replace(\"\\n\", \"\").replace(\"\\r\", \"\").replace(\"N\", \"A\") for seq in batch]\n",
    "            \n",
    "            inputs = self.tokenizer(\n",
    "                batch, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                max_length=1000\n",
    "            ).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs, output_hidden_states=True)\n",
    "                \n",
    "                # Mean Pooling\n",
    "                last_hidden_state = outputs.hidden_states[-1]\n",
    "                attention_mask = inputs[\"attention_mask\"].unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "                \n",
    "                sum_embeddings = torch.sum(last_hidden_state * attention_mask, 1)\n",
    "                sum_mask = torch.clamp(attention_mask.sum(1), min=1e-9)\n",
    "                mean_embeddings = sum_embeddings / sum_mask\n",
    "                \n",
    "                # The 50m model has hidden_size=512. We need exactly 768 dimensions.\n",
    "                # We will pad with zeros to reach 768 dimensions.\n",
    "                current_dim = mean_embeddings.shape[1]\n",
    "                target_dim = 768\n",
    "                \n",
    "                if current_dim < target_dim:\n",
    "                    padding = torch.zeros((mean_embeddings.shape[0], target_dim - current_dim), device=self.device)\n",
    "                    mean_embeddings = torch.cat([mean_embeddings, padding], dim=1)\n",
    "                elif current_dim > target_dim:\n",
    "                    mean_embeddings = mean_embeddings[:, :target_dim]\n",
    "                \n",
    "                # Yield float32 numpy arrays\n",
    "                yield mean_embeddings.cpu().numpy().astype(np.float32)\n",
    "\n",
    "# Initialize Embedder\n",
    "embedder = LargeScaleEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420cf02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Data-Ops: Step 3 - Merge vectors and Export\n",
    "print(\"Starting large-scale embedding generation...\")\n",
    "\n",
    "all_vectors = []\n",
    "# Using the generator\n",
    "for batch_vectors in embedder.embedding_generator(df_sequences[\"Sequence\"].tolist(), batch_size=64):\n",
    "    all_vectors.append(batch_vectors)\n",
    "\n",
    "# Concatenate all batches\n",
    "final_vectors = np.concatenate(all_vectors, axis=0)\n",
    "print(f\"Generated vectors shape: {final_vectors.shape}\")\n",
    "\n",
    "# Merge vectors with the full taxonomic metadata\n",
    "# We convert the 2D numpy array into a list of 1D arrays for Parquet storage\n",
    "df_sequences[\"Vector\"] = list(final_vectors)\n",
    "\n",
    "output_file = \"reference_atlas_100k.parquet\"\n",
    "df_sequences.to_parquet(output_file, engine=\"pyarrow\")\n",
    "\n",
    "print(f\"SUCCESS: Atlas saved to {output_file}.\")\n",
    "print(f\"Total records: {len(df_sequences)}\")\n",
    "print(f\"Quality Check Passed (>300bp): {df_sequences['Quality_Check'].sum()}\")\n",
    "print(\"Ready for LanceDB ingestion.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
