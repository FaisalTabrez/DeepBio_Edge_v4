{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acb0298f",
   "metadata": {},
   "source": [
    "# DeepBio-Scan: Large-Scale Atlas Seeding (N=100,000)\n",
    "## Phase 1: Reference Atlas Generation\n",
    "\n",
    "**Personas Active:**\n",
    "- `@Embedder-ML` (Model Logic & Inference)\n",
    "- `@Data-Ops` (Data Pipeline & Parquet Export)\n",
    "\n",
    "**Hardware Target:** Google Colab T4 GPU (or better)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c052488a",
   "metadata": {},
   "source": [
    "# DeepBio-Scan: Colab Seeding Phase\n",
    "## Phase 1: Reference Atlas Generation\n",
    "\n",
    "**Personas Active:**\n",
    "- `@Embedder-ML` (Model Logic & Inference)\n",
    "- `@Data-Ops` (Data Pipeline & Parquet Export)\n",
    "\n",
    "**Hardware Target:** Google Colab T4 GPU (or better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1649474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Data-Ops: Dependency Setup\n",
    "# Fix: Uninstall torch_xla to prevent ABI conflicts with updated libraries on GPU runtimes\n",
    "!pip uninstall -y torch_xla\n",
    "!pip install --upgrade transformers==4.40.2 pandas pyarrow duckdb lancedb accelerate biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31221fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig\n",
    "\n",
    "# @Embedder-ML: GPU Acceleration Check\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on: {device.upper()}\")\n",
    "if device == \"cpu\":\n",
    "    print(\"WARNING: GPU not detected. Embedding will be slow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689604ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColabEmbedder:\n",
    "    def __init__(self, model_name=\"InstaDeepAI/nucleotide-transformer-v2-50m-multi-species\"):\n",
    "        \"\"\"\n",
    "        @Embedder-ML: Initializes the Nucleotide Transformer with GPU optimization.\n",
    "        \"\"\"\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        print(f\"Initializing Model: {model_name}...\")\n",
    "        \n",
    "        # Load Config\n",
    "        config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "        \n",
    "        # @Embedder-ML: Size Auto-Detection\n",
    "        # The 50M model uses hidden_size=512, while larger ones use 768 or 1280.\n",
    "        # We assume the config on HuggingFace is now correct (intermediate_size=2048 for 50M).\n",
    "        self.hidden_dim = getattr(config, \"hidden_size\", 512)\n",
    "        print(f\"Auto-Detected Hidden Dimension: {self.hidden_dim}\")\n",
    "            \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(\n",
    "            model_name, \n",
    "            config=config,\n",
    "            trust_remote_code=True\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        print(\"Model successfully loaded on GPU.\")\n",
    "\n",
    "    def get_embeddings(self, sequences, batch_size=16):\n",
    "        \"\"\"\n",
    "        Generates embeddings with Shape Defense logic.\n",
    "        \"\"\"\n",
    "        all_embeddings = []\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in range(0, len(sequences), batch_size):\n",
    "            batch = sequences[i:i+batch_size]\n",
    "\n",
    "            # @Embedder-ML: Sequence Cleaner (Biological Reality)\n",
    "            # Real sequences from NCBI often contain whitespaces, newlines, or \"N\" (unknown) characters.\n",
    "            batch = [seq.upper().replace(\"\\n\", \"\").replace(\"\\r\", \"\") for seq in batch]\n",
    "            # Replace 'N' with 'A' (neutral assumption) for model stability\n",
    "            batch = [seq.replace(\"N\", \"A\") for seq in batch]\n",
    "            \n",
    "            # Tokenization\n",
    "            inputs = self.tokenizer(\n",
    "                batch, \n",
    "                return_tensors=\"pt\", \n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                max_length=1000\n",
    "            ).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs, output_hidden_states=True)\n",
    "                \n",
    "                # Mean Pooling Strategy\n",
    "                last_hidden_state = outputs.hidden_states[-1]\n",
    "                attention_mask = inputs[\"attention_mask\"].unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "                \n",
    "                sum_embeddings = torch.sum(last_hidden_state * attention_mask, 1)\n",
    "                sum_mask = torch.clamp(attention_mask.sum(1), min=1e-9)\n",
    "                mean_embeddings = sum_embeddings / sum_mask\n",
    "                \n",
    "                # @Embedder-ML: Shape Defense\n",
    "                # Verify dimensions are (Batch_Size, Hidden_Dim)\n",
    "                if mean_embeddings.shape[1] != self.hidden_dim:\n",
    "                    raise ValueError(f\"Shape Mismatch! Expected {self.hidden_dim}, got {mean_embeddings.shape[1]}\")\n",
    "                \n",
    "                all_embeddings.append(mean_embeddings.cpu().numpy())\n",
    "        \n",
    "        if not all_embeddings:\n",
    "            return np.empty((0, self.hidden_dim))\n",
    "        \n",
    "        # @Data-Ops: Vector Type Casting (Memory Optimization)\n",
    "        # LanceDB and 32GB USB drive performance optimization (float32)\n",
    "        return np.concatenate(all_embeddings, axis=0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad9829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Data-Ops: File Upload Utility\n",
    "# Run this cell to upload 'Expedition_DeepSea_Batch.fasta' from your local machine.\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(\"Initiating Transport Uplink...\")\n",
    "    uploaded = files.upload()\n",
    "    for fn in uploaded.keys():\n",
    "        print(f\"Received artifact: {fn} ({len(uploaded[fn])} bytes)\")\n",
    "except ImportError:\n",
    "    print(\"Not running in Google Colab. Skipping upload widget.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ca94f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Data-Ops: Real Data Ingestion\n",
    "# Upload your real FASTA or Parquet to Colab, then run:\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "input_path = \"real_deepsea_data.parquet\" \n",
    "\n",
    "if os.path.exists(input_path):\n",
    "    print(f\"Found real dataset: {input_path}\")\n",
    "    df = pd.read_parquet(input_path)\n",
    "    print(f\"Loaded {len(df)} real sequences.\")\n",
    "else:\n",
    "    print(\"Input file not found. Falling back to synthetic test.\")\n",
    "    \n",
    "    # @Data-Ops: Data Ingestion Simulation\n",
    "    # In a real scenario, this would query OBIS or NCBI APIs.\n",
    "    # Here we create a synthetic dataset for the demo seeding.\n",
    "    print(\"Generating Synthetic Deep-Sea Dataset for Demo...\")\n",
    "    synthetic_data = {\n",
    "        \"id\": [f\"seq_{i}\" for i in range(100)],\n",
    "        \"species\": [\"Grimpoteuthis sp.\" if i % 2 == 0 else \"Bathynomus giganteus\" for i in range(100)],\n",
    "        \"sequence\": [\"AGTC\" * 250 for _ in range(100)] # Placeholder DNA\n",
    "    }\n",
    "    df = pd.DataFrame(synthetic_data)\n",
    "    print(f\"Loaded {len(df)} sequences for processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34d4f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Data-Ops: FASTA to Parquet Conversion\n",
    "# If you uploaded a FASTA file (e.g., from the DeepBio fetch script), convert it here.\n",
    "# Otherwise, skip this cell if using a pre-made Parquet.\n",
    "\n",
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "\n",
    "fasta_input = \"Expedition_DeepSea_Batch.fasta\" # Upload this file to Colab first\n",
    "\n",
    "if os.path.exists(fasta_input):\n",
    "    print(f\"Processing FASTA: {fasta_input}\")\n",
    "    records = []\n",
    "    for record in SeqIO.parse(fasta_input, \"fasta\"):\n",
    "        # Header format from fetcher: >Accession | Species | Depth:XM\n",
    "        parts = record.description.split(\"|\")\n",
    "        \n",
    "        # Robust parsing\n",
    "        species = parts[1].strip() if len(parts) > 1 else \"Unknown\"\n",
    "        seq_id = parts[0].strip()\n",
    "        \n",
    "        records.append({\n",
    "            \"id\": seq_id,\n",
    "            \"species\": species,\n",
    "            \"sequence\": str(record.seq)\n",
    "        })\n",
    "        \n",
    "    df = pd.DataFrame(records)\n",
    "    print(f\"Converted {len(df)} FASTA records to DataFrame.\")\n",
    "    \n",
    "    # Overwrite the input logic for the next cell\n",
    "    input_path = \"real_deepsea_data.parquet\"\n",
    "    df.to_parquet(input_path)\n",
    "    print(\"Saved to temporary Parquet for embedding pipeline.\")\n",
    "else:\n",
    "    print(f\"FASTA file {fasta_input} not found. Skipping conversion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac08d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Data-Ops: Pipeline Execution\n",
    "\n",
    "# 1. Initialize Embedder\n",
    "embedder = ColabEmbedder()\n",
    "\n",
    "# 2. Generate Embeddings\n",
    "print(\"Starting embedding process...\")\n",
    "vectors = embedder.get_embeddings(df[\"sequence\"].tolist(), batch_size=32)\n",
    "\n",
    "# 3. Merge with Metadata\n",
    "df[\"vector\"] = list(vectors)\n",
    "print(f\"Vectors Generated. Shape: {vectors.shape}\")\n",
    "\n",
    "# 4. Save to Parquet (Optimized for LanceDB)\n",
    "output_file = \"deepbio_reference_atlas.parquet\"\n",
    "df.to_parquet(output_file)\n",
    "print(f\"SUCCESS: Atlas saved to {output_file}. Ready for USB Transfer.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
